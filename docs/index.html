<!doctype html>
<meta charset="utf-8">
<script src="template.js"></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<link href="article.css" rel="stylesheet">


<script type="text/front-matter">
  title: "Caps Lock"
  description: "Exploring Generative Adversarial Netorks with Capsule Discrimination"
  authors:
  - Kevin Jiao: https://kevinjiao.me
  - Pasha Minkovsky: http://google.com
  - Franklin Rice: http://google.com
  - Angelina Wang: http://angelina-wang.github.io
  affiliations:
  - UC Berkeley: https://eecs.berkeley.edu/
  - UC Berkeley: https://eecs.berkeley.edu/
  - UC Berkeley: https://eecs.berkeley.edu/
  - UC Berkeley: https://eecs.berkeley.edu/
</script>
<dt-article>
  <h1>Caps Lock</h1>
  <h2>Generative Adversarial Networks with Capsule Networks</h2>
  <h3> Overview </h3>
  <p>
    In this blogpost, we will discuss how we leveraged a capsule network to serve as the discriminator in a Generative Adversarial
    architecture. We use our model on a variety of similar datasets in the MNIST family, hoping that the vector activations
    produced by a capsule network will be able to provide more nuanced feedback to the generator, and thus lead to improved
    generated images. We are especially interested in transformed and rotated images, which capsule networks perform
    particularly well with. We achieve significant improvements over the baseline model of a deep convolutional
    GAN. To further our analysis, we pinpoint certain areas in the latent space in which certain image transformations or patterns arise. 
  </p>
  <dt-byline></dt-byline>
  <h3>Background</h3>
  <p>
    Two recently popularized, innovative models are GANs (Generative Adversarial Networks)
    <dt-cite key="iangan"></dt-cite>, and CapsNets (capsule networks)
    <dt-cite key="hintoncaps"></dt-cite>. In this paper, we will explain how we integrate the two into a CAPSGAN. 
    The strength of a Capsule Network lies in its dynamic routing capabilities, which direct the output of a neuron based on
    its cosine similarity with other neurons. Capsule networks also internally represent objects as vectors, providing far more
    expressive power than a traditional CNN. This representation also allows for rotational invariance, which we test against a
    rotated MNIST dataset.
  </p>
  <h4>GAN</h4>
  <p>Generative adversarial networks are a model used to generate images from a learned distribution. They are composed of two
    components: a generator that learns to generate images given a vector sampled from the latent distribution, and a discriminator
    that learns to discriminate between generated images and real ones from the dataset. The two components of this model
    play a minimax game, by the end of which the generator is able to generate images from the data distribution by randomly
    sampling from the latent space. Traditionally, architectures such as a deep convolutional neural network are used for
    both components, but we propose using a capsule network for the discriminator.</p>
  <h4>CapsNet</h4>
  <p>Capsule networks
    <dt-cite key="capsgan"></dt-cite> are a recent development that addresses some of the weaknesses of traditional
    convolutional neural networks. In this model, each capsule is responsible for a region of the input, and outputs an array
    of values representing it. By using vectors activations rather than scalars, capsules are able to send much more information onto the next layer about the kinds of features captured in the image. Both the direction and magnitude of the vector contain important information about the features. Capsule networks also use a different kind of non-linearity then traditional CNN's called squashing,
    which scales the vector so that it's magnitude denotes a probability value. Because a capsule contains many pieces of
    information about a location, a procedure called routing determines what is sent up to the next layer based off what
    is believed to be more important.
    <figure>
      <img src="capsnet-diagram.png">
      <figcaption>Capsule network visualization</figcaption>
    </figure>
  </p>
  <h3>Our approach</h3>
  <h4>Data</h4>
  <p>
    We chose to investigate the nuances of capsule representations by focusing on various variations of the MNIST dataset. These
    included:
    <ol>
      <li>Original MNIST, which we obtained through the built-in PyTorch TorchVision datasets
        <figure>
          <img src="mnist_real.png">
          <figcaption>MNIST examples</figcaption>
        </figure>
      </li>
      <li> Extended MNIST (EMNIST) which contains both letters and numbers, each of which is rotated randomly, and for which we
        also utilized the TorchVision datasets;
        <figure>
          <img src="emnist_real.png">
          <figcaption>EMNIST examples</figcaption>
        </figure>
      </li>
      <li> Rotated MNIST, which we created ourselves by applying a random rotation between 0 and 360 degrees
        <figure>
          <img src="mnist_rot_real.png">
          <figcaption>Rotated MNIST examples</figcaption>
        </figure>
      </li>
    </ol>
    <p>
      Our hope was that, by looking more deeply into these similar datasets, we would be able to discern more detailed learning
      points than if we simply obtained proof-of-concept results on a larger breadth of datasets.</p>
  </p>

  <h4>Baseline</h4>
  <p>We chose to also perform the same experiments using the Deep Convolutional Generative Adversarial Network (DCGAN)
    <dt-cite key="dcgan"></dt-cite> developed by Radford et.al. as a baseline. This GAN has a very similar architecture to ours, the difference
    of emphasis being that its discriminator utilizes a deep convolutional network instead of a capsule network.</p>
  <figure>
    <img src="https://wp-cdn-2.s3.amazonaws.com/wp-content/uploads/2017/09/deep_convolutional_generative_adversarial_network1.png">
    <figcaption>DCGAN architecture </figcaption>
  </figure>
  <h4>Model</h4>
  <p>In our implementation, we propose replacing the discrimination component of the GAN with a CapsNet. CapsNets have been
    shown to have remarkable classification properties, which we leverage in the training process for the generator. This
    increased scrutiny on the generator results in more robust and impressive images. For the generator, we used the same
    model as the DCGAN but make numerous architectural changes we felt would help the generator as well as in regards to
    outputting a 28x28 image rather than a 64x64 one that it originally generated. We tried out different layers in which
    to add a singular Mean Pooling Layer, finally settling on the last layer as that yielded the best results. For the discriminator,
    we modified a CapsNet originally used for MNIST classification so that rather than outputting the probabilities an image
    belonged to one of the ten labels, there is a single output vector whose magnitude is the probability of the image being
    real or generated. The squash activation at the end ensures the output can be used as a probability measure, and thus
    our discriminator could be plugged into our GAN model.</p>

  <h3>Results</h3>
  <p>Our trained generator was able to generate sufficiently similar images for all three of our datasets.
    <figure>
      <img src="mnist-results.png">
      <figcaption>MNIST generated samples</figcaption>
    </figure>
    <figure>
      <img src="emnist-results.png">
      <figcaption>EMNIST generated samples</figcaption>
    </figure>
    <figure>
      <img src="mnist-rot-results.png">
      <figcaption>Rotated MNIST generated samples</figcaption>
    </figure>
  </p>
  <p>We also evaluated the strength of our GAN with the Generative Adversarial Metric proposed by Im et. al.
    <dt-cite key="ganmetric"></dt-cite>
    Our network has a r_sample score of TODO and a r_test score of TODO, compared to the baseline score of.
  </p>
  <h3>Training</h3>
  <figure>
    <img src="mnist_train_hist.png">
    <figcaption>MNIST training loss history</figcaption>
  </figure>
  <figure>
    <img src="emnist_train_hist.png">
    <figcaption>EMNIST training loss history</figcaption>
  </figure>
  <figure>
    <img src="mnist_rot_train_hist.png">
    <figcaption>Rotated MNIST training loss history</figcaption>
  </figure>

  <h3>Experiments</h3>
  <h4>Running with Different Steps</h4>
  <p>
    Other things we tried out to train better images, in addition to the extensive architecture and hyperparameter tuning discussed above, include running the generator and discriminator for different amounts of steps. The intuition behind trying this was we realized that because we are using a capsule net as a discriminator rather than a more simple convolutional network, our discriminator might be more powerful than our generator, and thus rather than training in parallel with the generator, might outpace it leading to poor images. To resolve this, we tried training our generator for n steps to every 1 of the discriminator's, however we found that this didn't work very well as this led to quick mode collapse. Thus, most of our results actually came from just running 1 step on each model in pace with each other, because we used a vanilla-enough capsule network as the discriminator that it was able to be trained in parallel to the generator. Neither model “outpaced” the other, so the feedback was never saturated.
  </p>

  <h4>Possible Rotational Advantages</h4>
  <p>
    Another thing we tried to see if we could isolate the effects of the capsule network as a discriminator over a convolutional network as one was taking the discriminators from both models trained on regular MNIST, and running them against test images from the rotated MNIST dataset, hoping to see if the CapsGan discriminator would classify them as real, whereas the DCGAN discriminator would classify them as fake because the convolutional layers were unable to deal with the rotational invariance. However, this did not work as expected because both discriminators classified all of the images as fake. However, this may be because the rotated images seem to have a sort of blur around the edges, due to the fact that rotation of a $28x28$ image does not cause the images to remain as smooth as they did before. Just from viewing the MNIST dataset regular and rotated, one can spot the difference, so we hypothesize that the discriminator is unable to deal with the rotation for this reason, rather than a lack of rotational invariance.
  </p>

  <h3>Visualization</h3>
  <p>
  By perturbing a particular index of our latent vector, we were able to visualize how values at that index affected the output image.
  The following gif's were created from the generator output of a fixed latent vector, with one index changing by a fixed number.
  
  In this example, we discovered a scalar that corresponded to the length of the left leg of a 4.
</p>
  <figure class="insert-left">
      <img src="4.gif">
      <figcaption>Notice the left vertical portion changing</figcaption>
    </figure>

  <p>This next one has an affect on how sharp the corners of a 5 are.</p>
</p>
<br>
<figure class="insert-left">
  <img src="5.gif">
  <figcaption>See how the entire figure goes from sharp to curved</figcaption>
</figure>
<p>One other example is this one, where changing a single index effects how large the lower loop of a 6 is. At the most extreme it is non-existent,
  and at the other it is clearly a 6.
</p>
<figure class="insert-left">
    <img src="6.gif">
    <figcaption>The transformation from 1 to 6</figcaption>
</figure>

<figure>
  <img src="capsgan_gen_array_1_row.png" class="full-img">
  <img src="dcgan_gen_array_1_row.png" class="full-img">
  <figcaption>Here we can see two similar gifs unrolled. The top sequence is generated by the generator trained against our capsule disriminator, while the lower sequence is from by a standard DCGAN generator we trained using the same framework and number of epochs.></figcaption>

</figure>
<p>Interestingly, <i>even though the generator does not contain any capsules itself, we can see that our CapsGAN generator has learned a representation characteristic of capsules</i>. Namely, the varying of the feature, rather than causing somewhat smooth movement between two decent-looking numbers as is seen in the DCGAN case, seems to correspond to the stroke heaviness of the number. At either extreme we see very heavy numbers, which become lighter with a less extreme feature value (-1 and 1 are at the extremes), eventually switching classes in the middle. This is in line with Hinton's explanation of capsule networks in his original paper <dt-cite key="hintoncaps"></dt-cite>, where he found a single capsule feature to correspond to various image features, like rotation, skew, stroke, etc.
</p>
<p> As an aside, it is also interesting to note that in this test we uncovered a latent space vector which our fully trained DCGAN generator mapped to a meaningless checkerboard pattern. This is something that we discovered in the training of our capsule net; in the early epochs of some runs, our generator would always output results looking like those middle cells. We attributed this to a combination of mode collapse and particularly non-robust instantiation requirements. It is nice to see that the DCGAN also struggles with this, and is unable to eliminate these "failure pockets" from the latent space entirely (we checked 40 input features and never encountered this problem with our fully trained CapsGAN)
  </p>
<p> All of our unrolled GIFs can be viewed <a href="capsgan_gen_array_2.png">here</a> for CapsGAN, and <a href="dcgan_gen_array.png">here</a> for DCGAN. 20 of the 100 input dimensions are represented in these disgrams.
</p>

<h4>Synthetic Dataset</h4>
<p>We also created a little dataset of our own to see if we could isolate the features within the capsule discriminator that represent certain parameters of the input digit, like rotation, skew, scale, translation, stroke thickness, and stretch. For example, we created 7 different images of the same 4, but with various degrees of rotation.
</p>
<p>
Here are some of the image sequences we made to test:
<div class="banner">
  <img src="transformation_gifs/2_rot.gif" class="inline-img">
  <img src="transformation_gifs/4_stroke.gif" class="inline-img">
  <img src="transformation_gifs/8_trans.gif" class="inline-img">
  <img src="transformation_gifs/3_stretch.gif" class="inline-img">
  <img src="transformation_gifs/8_skew.gif" class="inline-img">
  <img src="transformation_gifs/0_scale.gif" class="inline-img">
  <img src="transformation_gifs/4_rot.gif" class="inline-img">
</div>

<br>

<p> While our Capsule Discriminator was performing a forward pass on each of these images, we extracted:
  <ul>
    <li>The PrimaryCaps layer, which is of dimension 8 x 1152, and is the second layer of three in the discriminator, but the first capsule layer. </li>
    <li>The DigitCaps layer, which is of dimension 16, and is the third and final layer in the capsule network in the discriminator.
  </ul>
</p>
<p>
  One of the features of capsule networks is that the activation vectors have magnitude equal to 1, regardless of the dimensionality. Thus, in the 9216 dimensional PrimaryCaps vector, most of the components were very small. This is one of the colloquially named "curses of dimensionality", whereby most components of a vector go to 0 as the dimension of the vector increases and the magnitude is kept constant. 

The most relevant components would have higher values than the rest, so we wanted to isolate them. We zero’d all components whose values were less than 60% of the max value and more than 60% of the (negative) min value for the entire capsule network. This isolated components whose activations were most relevant.

Then, to find which components map to which features, we visualized the PrimaryCaps layer as a 96 x 96 image, and the DigitCaps layer as a 16x1 image, and looked at the <i>difference</i> between the capsule activations of 2 images with varying degrees of the same transformation. For example, subtracting the capsule activation for a 4 with 45˚ rotation from the activation of a 4 with 0˚ rotation yields the following:
</p>
<div class="banner">
  <img src="4_diff_example_96.png" class="inline-img">
  <img src="4_diff_example_16.png" class="inline-img" style="margin-top: 140px; margin-left: 20px;">
</div>
<figure class="caponly"><figcaption>The difference between the first two activations in the "rotated 4" series, with lesser values zero'd. The left image corresponds to the 8 x 1152 "PrimaryCaps" layer, while the right image comes from the 16x1 "DigitCaps" layer.</figcaption></figure>

<p>
Doing this for each pair of successive images yields a clear picture of which components are consistently activated as transformations take place. In theory, if we do indeed have a feature representing rotation, each progressive rotation step should show a significant activation there.
</p>

<figure class="insert-left nobot">
    <img src="transformation_gifs/4_rot.gif" class="inline-img">
</figure>
<figure class="insert-left notop">
  <img src="96x96_gifs/4_rot.gif" class="inline-img">
  <figcaption>The sequence of activation differences for the rotating 4.</figcaption>
</figure >
<br>
<p>The most evident components are in the rotation of the 4. The yellow pixels represent positive values for a component and dark blue represents negative. Teal is 0. The main activations for the rotation can be found in bands. What’s interesting is that each band’s components flip between dark blue and yellow depending on the orientation of the image. For example, during the cycle of the gif the band in the lower left corner will have dark blue pixels on the left along with yellow pixels on the right, and then yellow pixels on the left and blue pixels on the right when the image rotates 180˚. This mirroring behavior is evident in almost every active band for this transformation. These components of the capsule network are representing the rotation of the image. 
</p>

<figure class="insert-left nobot">
    <img src="transformation_gifs/0_scale.gif" class="inline-img">
</figure>
<figure class="insert-left notop">
  <img src="96x96_gifs/0_size.gif" class="inline-img">
  <figcaption>The sequence of activation differences for the shrinking 0.</figcaption>
</figure >
<br>
<p>Another interesting thing we discovered is the row of neurons in the bottom left corner (0-22, 95) and are active in all of the transformations. Interpreting the exact meaning of the band is difficult because there are other components that are activated in every capsule layer. However, we can still glean meaning from trends within a transformation. For the 0 transformation, the band goes from negative on the left side to positive on the right side as the 0 shrinks. For the 4 stroke thickness, the activations move to the right on the band as the stroke gets thicker. For the 8 skew, the trend is the opposite, the activation skips to the left as the 8 is skewed. Another interesting point about the 8 skew activations is that this band is the only 1 of 4 bands that consistently appears in each transformation (the other two are (3-11, 50), (5-22, 38), and (60-70, 20). The 8 translation gif also displays some interesting patterns. In the  (0-22, 95) band there is a checkered pattern that moves to the right as a unit as the 8 is translated. The other activation bands are fairly homogenous (either all positive components or all negative components).
</p>

<figure class="insert-left nobot">
    <img src="transformation_gifs/8_trans.gif" class="inline-img">
</figure>
<figure class="insert-left notop">
  <img src="96x96_gifs/8_trans.gif" class="inline-img">
  <figcaption>The sequence of activation differences for the shrinking 0.</figcaption>
</figure >
<br>
<p>Overall, analyzing the capsule network shows that the layers are learning rich representations of the MNIST digits, and one which would not be expected from a purely convoutional network. These nuanced representations guide the discriminator's actions, and as we saw above, are passed on to the generator's representation through the backpropagation process.
</p>

<br><br><br><br><br><br><br><br><br><br>


</dt-article>

<dt-appendix>
</dt-appendix>

<script type="text/bibliography">
  @article{hintoncaps,
    title={Dynamic Routing Between Capsules},
    author={Sara Sabour, Nicholas Frosst, Geoffrey E Hinton},
    journal={arXiv:1710.09829},
    year={2017},
    url={https://arxiv.org/pdf/1710.09829.pdf}
  }

  @article{iangan,
    title={Generative Adversarial Networks},
    author={Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio},
    journal={arXiv:1406.2661},
    year={2014},
    url={https://arxiv.org/pdf/1406.2661.pdf}
  }

  @article{capsgan,
    title={CapsuleGAN: Generative Adversarial Capsule Network},
    author={Ayush Jaiswal, Wael AbdAlmageed, Yue Wu, Premkumar Natarajan},
    journal={arXiv:1802.06167},
    year={2018},
    url={https://arxiv.org/pdf/1802.06167.pdf}
  }

  @article{dcgan,
    Author = {Alec Radford and Luke Metz and Soumith Chintala},
    Title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
    Year = {2015},
    journal = {arXiv:1511.06434},
    url={https://arxiv.org/pdf/1511.06434.pdf}
    }
  @article{ganmetric,
      Author = {Daniel Jiwoong Im and Chris Dongjoo Kim and Hui Jiang and Roland Memisevic},
      Title = {Generating images with recurrent adversarial networks},
      Year = {2016},
      journal = {arXiv:1602.05110},
      url = {https://arxiv.org/pdf/1602.05110.pdf}
      }
</script>
