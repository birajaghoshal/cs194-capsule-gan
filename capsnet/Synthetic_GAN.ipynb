{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pickle\n",
    "import imageio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from model import Net\n",
    "import argparse\n",
    "import utils\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "fixed_z_ = torch.randn((5 * 5, 100)).view(-1, 100, 1, 1)    # fixed noise\n",
    "if torch.cuda.is_available():\n",
    "    fixed_z_.cuda()\n",
    "fixed_z_ = Variable(fixed_z_)\n",
    "def show_result(num_epoch, show = False, save = False, path = 'result.png', isFix=False):\n",
    "    z_ = torch.randn((5*5, 100)).view(-1, 100, 1, 1)\n",
    "    if torch.cuda.is_available():\n",
    "        z_.cuda()\n",
    "    z_ = Variable(z_, volatile=True)\n",
    "\n",
    "    G.eval()\n",
    "    if isFix:\n",
    "        test_images = G(fixed_z_)\n",
    "    else:\n",
    "        test_images = G(z_)\n",
    "    G.train()\n",
    "\n",
    "    size_figure_grid = 5\n",
    "    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n",
    "    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
    "        ax[i, j].get_xaxis().set_visible(False)\n",
    "        ax[i, j].get_yaxis().set_visible(False)\n",
    "\n",
    "    for k in range(5*5):\n",
    "        i = k // 5\n",
    "        j = k % 5\n",
    "        ax[i, j].cla()\n",
    "        ax[i, j].imshow(test_images[k, 0].cpu().data.numpy(), cmap='gray')\n",
    "\n",
    "    label = 'Epoch {0}'.format(num_epoch)\n",
    "    fig.text(0.5, 0.04, label, ha='center')\n",
    "    plt.savefig(path)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_type, dict_file):\n",
    "    state_dict = torch.load(dict_file, map_location=lambda storage, loc: storage)\n",
    "\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k\n",
    "        if k[:7] == 'module.':\n",
    "            name = k[7:] # remove `module.`\n",
    "#         if name[:2] == 'fc':\n",
    "#             name = 'decoder.' + name\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    model = None\n",
    "    if model_type == 'g':\n",
    "        model = generator(128)\n",
    "    elif model_type == 'd':\n",
    "        model = Net(num_conv_in_channel=args.num_conv_in_channel,\n",
    "                    num_conv_out_channel=args.num_conv_out_channel,\n",
    "                    num_primary_unit=args.num_primary_unit,\n",
    "                    primary_unit_size=args.primary_unit_size,\n",
    "                    num_classes=args.num_classes,\n",
    "                    output_unit_size=args.output_unit_size,\n",
    "                    num_routing=args.num_routing,\n",
    "                    use_reconstruction_loss=args.use_reconstruction_loss,\n",
    "                    regularization_scale=args.regularization_scale,\n",
    "                    input_width=args.input_width,\n",
    "                    input_height=args.input_height,\n",
    "                    cuda_enabled=args.cuda)\n",
    "    elif model_type == 'b_g':\n",
    "        model = base_generator(128)\n",
    "    elif model_type == 'b_d':\n",
    "        model = base_discriminator(128)\n",
    "\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class Synthetic_MNIST_LOADER():\n",
    "    def __init__(self):\n",
    "        self.folder = '../synthetic_mnist/'\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.folder + str(index+1) + '.jpg'\n",
    "        image = Image.open(img_id)\n",
    "        \n",
    "        ### taking image from 26x26 to 28x28\n",
    "        new_image = Image.new(\"RGB\", (28,28)) \n",
    "        new_image.paste(image, (1, 1))\n",
    "        \n",
    "        image = new_image.convert('L')\n",
    "        \n",
    "        image_tensor = self.to_tensor(image)\n",
    "        path = img_id\n",
    "        return (image_tensor, path)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=256, dataset='mnist', epochs=10, input_height=28, input_width=28, is_training=1, log_interval=10, lr=0.01, no_cuda=True, num_classes=1, num_conv_in_channel=1, num_conv_out_channel=256, num_primary_unit=8, num_routing=3, output_unit_size=16, primary_unit_size=1152, regularization_scale=0.0005, seed=42, test_batch_size=128, threads=4, use_reconstruction_loss=True, weights=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb1aaa30e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb1a9e7f518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb1a994cdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb1aa4ff828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb1a961c240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb1a8856a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb1a9de7a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv=['']\n",
    "parser = argparse.ArgumentParser(description='Example of Capsule Network')\n",
    "parser.add_argument('--epochs', type=int, default=10,\n",
    "                    help='number of training epochs. default=10')\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='learning rate. default=0.01')\n",
    "parser.add_argument('--batch-size', type=int, default=256,\n",
    "                    help='training batch size. default=128')\n",
    "parser.add_argument('--test-batch-size', type=int,\n",
    "                    default=128, help='testing batch size. default=128')\n",
    "parser.add_argument('--log-interval', type=int, default=10,\n",
    "                    help='how many batches to wait before logging training status. default=10')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training. default=false')\n",
    "parser.add_argument('--threads', type=int, default=4,\n",
    "                    help='number of threads for data loader to use. default=4')\n",
    "parser.add_argument('--seed', type=int, default=42,\n",
    "                    help='random seed for training. default=42')\n",
    "parser.add_argument('--num-conv-out-channel', type=int, default=256,\n",
    "                    help='number of channels produced by the convolution. default=256')\n",
    "parser.add_argument('--num-conv-in-channel', type=int, default=1,\n",
    "                    help='number of input channels to the convolution. default=1')\n",
    "parser.add_argument('--num-primary-unit', type=int, default=8,\n",
    "                    help='number of primary unit. default=8')\n",
    "parser.add_argument('--primary-unit-size', type=int,\n",
    "                    default=1152, help='primary unit size is 32 * 6 * 6. default=1152')\n",
    "parser.add_argument('--num-classes', type=int, default=1,\n",
    "                    help='number of digit classes. 1 unit for one MNIST digit. default=10')\n",
    "parser.add_argument('--output-unit-size', type=int,\n",
    "                    default=16, help='output unit size. default=16')\n",
    "parser.add_argument('--num-routing', type=int,\n",
    "                    default=3, help='number of routing iteration. default=3')\n",
    "parser.add_argument('--use-reconstruction-loss', type=utils.str2bool, nargs='?', default=True,\n",
    "                    help='use an additional reconstruction loss. default=True')\n",
    "parser.add_argument('--regularization-scale', type=float, default=0.0005,\n",
    "                    help='regularization coefficient for reconstruction loss. default=0.0005')\n",
    "parser.add_argument('--dataset', help='the name of dataset (mnist, cifar10)', default='mnist')\n",
    "parser.add_argument('--input-width', type=int,\n",
    "                    default=28, help='input image width to the convolution. default=28 for MNIST')\n",
    "parser.add_argument('--input-height', type=int,\n",
    "                    default=28, help='input image height to the convolution. default=28 for MNIST')\n",
    "parser.add_argument('--is-training', type=int,\n",
    "                    default=1, help='Whether or not is training, default is yes')\n",
    "parser.add_argument('--weights', type=str,\n",
    "                    default=None, help='Load pretrained weights, default is none')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.no_cuda = True\n",
    "print(args)\n",
    "\n",
    "# Check GPU or CUDA is available\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# Get reproducible results by manually seed the random number generator\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "\n",
    "# training parameters\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "train_epoch = 20\n",
    "\n",
    "# data_loader\n",
    "img_size = 28#64\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Scale(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=transform),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "synth_loader = Synthetic_MNIST_LOADER()\n",
    "\n",
    "# network\n",
    "\n",
    "D = load_model('d', 'MNIST_CAPSGAN_FC_results/discriminator_param.pkl')\n",
    "\n",
    "if args.cuda:\n",
    "    print('Utilize GPUs for computation')\n",
    "    print('Number of GPU available', torch.cuda.device_count())\n",
    "    print('CUDNN version: ', torch.backends.cudnn.version())\n",
    "    # torch.backends.cudnn.benchmark = True\n",
    "    D = torch.nn.DataParallel(D).cuda()\n",
    "\n",
    "# results save folder\n",
    "if not os.path.isdir('synthetic_results'):\n",
    "    os.mkdir('synthetic_results')\n",
    "\n",
    "    \n",
    "def thresh(vals, t):\n",
    "    inds = vals < t\n",
    "    vals[inds] = 0\n",
    "    return vals\n",
    "\n",
    "sets = [(\"3_fade\", 1, 4), (\"8_trans\", 5, 10), (\"4_rot\", 11, 18), (\"0_size\", 19, 22), (\"8_skew\", 23, 28), (\"3_width\", 29, 34), (\"4_stroke\", 35, 38)]\n",
    "### ONLY SET THESE ###\n",
    "for name, start, end in sets:\n",
    "    #start = 5\n",
    "    #end = 10\n",
    "\n",
    "    net = end - start + 1\n",
    "    fig, ax = plt.subplots(3,net)\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    for i in range(3):\n",
    "        for j in range(net):\n",
    "            ax[i,j].get_xaxis().set_visible(False)\n",
    "            ax[i,j].get_yaxis().set_visible(False)\n",
    "            ax[i,j].cla()\n",
    "    values = []\n",
    "    counter = 1\n",
    "    for x_, path in synth_loader:\n",
    "        if counter >= start and counter <= end:\n",
    "\n",
    "            # print(str(x_.data.numpy()[0,1]))\n",
    "            x_.unsqueeze_(0)\n",
    "\n",
    "            D(x_)\n",
    "            feats = D.out_features\n",
    "            \n",
    "            feats = feats.squeeze()\n",
    "            feats = feats.unsqueeze(0)\n",
    "\n",
    "            values.append(feats.data.numpy())\n",
    "\n",
    "            #plt.imshow(feats.squeeze().data.reshape((96,96)))\n",
    "\n",
    "        if counter == 38:\n",
    "            for i in range(0,net):\n",
    "                ax[0, i].imshow(values[i], cmap='gray')\n",
    "            for i in range(1,net):\n",
    "                val = values[i-1] - values[i]\n",
    "                ax[1,i].imshow(thresh(val, np.max(val)/2), cmap='gray')\n",
    "            for i in range(1,net):\n",
    "                val = values[0] - values[i]\n",
    "                ax[2,i].imshow(thresh(val, np.max(val)/2), cmap='gray')\n",
    "\n",
    "            plt.savefig('figs/16_layer/' + name + '.png')\n",
    "            plt.clf()\n",
    "            np.save('figs/16_layer' + name, values)\n",
    "            break\n",
    "\n",
    "        counter += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
